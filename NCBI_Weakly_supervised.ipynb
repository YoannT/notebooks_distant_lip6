{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.text import huggingface_tokenize, regex_sentencize, partition_spans, encode_as_tag, split_into_spans, apply_substitutions, apply_deltas\n",
    "from nlstruct.dataloaders import *\n",
    "from nlstruct.collections import Dataset, Batcher\n",
    "from nlstruct.utils import merge_with_spans, normalize_vocabularies, factorize_rows, df_to_csr, factorize, torch_global as tg\n",
    "from nlstruct.modules.crf import BIODecoder, BIOULDecoder\n",
    "from nlstruct.environment import root, cached\n",
    "from nlstruct.train import seed_all\n",
    "from itertools import chain, repeat\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"nlstruct\")\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To debug the training, we can just comment the \"def run_epoch()\" and execute the function body manually without changing anything to it\n",
    "def extract_mentions(batcher, all_nets, max_depth=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batcher: Batcher \n",
    "        The batcher containing the text from which we want to extract the mentions (and maybe the gold mentions)\n",
    "    ner_net: torch.nn.Module\n",
    "    max_depth: int\n",
    "        Max number of times we run the model per sample\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Batcher\n",
    "    \"\"\"\n",
    "    pred_batches = []\n",
    "    n_mentions = 0\n",
    "    ner_net = all_nets[\"ner_net\"]\n",
    "    with evaluating(all_nets):\n",
    "        with torch.no_grad():\n",
    "            for batch_i, batch in enumerate(batcher['sentence'].dataloader(batch_size=batch_size, shuffle=False, sparse_sort_on=\"token_mask\", device=tg.device)):\n",
    "\n",
    "                mask = batch[\"token_mask\"]\n",
    "\n",
    "                res = all_nets[\"ner_net\"].forward(\n",
    "                    tokens=batch[\"token\"],\n",
    "                    mask=mask,\n",
    "                    return_loss = False,\n",
    "                )\n",
    "\n",
    "                spans = res['sampled_spans']\n",
    "\n",
    "                pred_batch = Batcher({\n",
    "                    \"mention\": {\n",
    "                        \"mention_id\": torch.arange(n_mentions, n_mentions+len(spans['span_doc_id']), device=device),\n",
    "                        \"begin\": spans[\"span_begin\"],\n",
    "                        \"end\": spans[\"span_end\"],\n",
    "                        \"ner_label\": spans[\"span_label\"],\n",
    "                        \"@sentence_id\": spans[\"span_doc_id\"],\n",
    "                    },\n",
    "                    \"sentence\": dict(batch[\"sentence\", [\"sentence_id\", \"doc_id\"]]),\n",
    "                    \"doc\": dict(batch[\"doc\"])}, \n",
    "                    check=False)\n",
    "\n",
    "                pred_batches.append(pred_batch)\n",
    "            \n",
    "    return Batcher.concat(pred_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define the training metrics\n",
    "metrics_info = defaultdict(lambda: False)\n",
    "flt_format = (5, \"{:.4f}\".format)\n",
    "metrics_info.update({\n",
    "    \"train_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"train_ner_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    #\"train_recall\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_rec\"},\n",
    "    #\"train_precision\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_prec\"},\n",
    "    \"train_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_f1\"},\n",
    "    \n",
    "    \"val_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"val_ner_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"val_label_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \n",
    "    \"val_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_f1\"},\n",
    "    \"val_3.1_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_3.1_f1\"},\n",
    "    \"val_3.2_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_3.2_f1\"},\n",
    "    \"val_macro_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_macro_f1\"},\n",
    "    \"val_sosy_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_sosy_f1\"},\n",
    "    \"val_pathologie_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_patho_f1\"},\n",
    "    \n",
    "    \"duration\": {\"format\": flt_format, \"name\": \"   dur(s)\"},\n",
    "    \"rescale\": {\"format\": flt_format},\n",
    "    \"n_depth\": {\"format\": flt_format},\n",
    "    \"n_matched\": {\"format\": flt_format},\n",
    "    \"n_targets\": {\"format\": flt_format},\n",
    "    \"n_observed\": {\"format\": flt_format},\n",
    "    \"total_score_sum\": {\"format\": flt_format},\n",
    "    \"lr\": {\"format\": (5, \"{:.2e}\".format)},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.utils import encode_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@cached\n",
    "def preprocess_train(\n",
    "    dataset,\n",
    "    max_sentence_length,\n",
    "    bert_name,\n",
    "    ner_labels=None,\n",
    "    unknown_labels=\"drop\",\n",
    "    vocabularies=None,\n",
    "    frag_merge=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: Dataset\n",
    "        max_sentence_length: int\n",
    "            Max number of \"words\" as defined by the regex in regex_sentencize (so this is not the nb of wordpieces)\n",
    "        bert_name: str\n",
    "            bert path/name\n",
    "        ner_labels: list of str \n",
    "            allowed ner labels (to be dropped or filtered)\n",
    "        unknown_labels: str\n",
    "            \"drop\" or \"raise\"\n",
    "        vocabularies: dict[str; np.ndarray or list]\n",
    "    Returns\n",
    "    -------\n",
    "    (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, dict[str; np.ndarray or list])\n",
    "        docs:      ('split', 'text', 'doc_id')\n",
    "        sentences: ('split', 'doc_id', 'sentence_idx', 'begin', 'end', 'text', 'sentence_id')\n",
    "        mentions:  ('ner_label', 'doc_id', 'sentence_id', 'mention_id', 'depth', 'text', 'mention_idx', 'begin', 'end')\n",
    "        tokens:    ('split', 'token', 'sentence_id', 'token_id', 'token_idx', 'begin', 'end', 'doc_id', 'sentence_idx')\n",
    "        deltas:    ('doc_id', 'begin', 'end', 'delta')\n",
    "        vocs: vocabularies to be reused later for encoding more data or decoding predictions\n",
    "    \"\"\"\n",
    "    print(\"Dataset:\", dataset)\n",
    "    mentions = dataset[\"mentions\"].rename({\"label\": \"ner_label\"}, axis=1)\n",
    "    if ner_labels is not None:\n",
    "        len_before = len(mentions)\n",
    "        unknown_ner_labels = list(mentions[~mentions[\"ner_label\"].isin(ner_labels)][\"ner_label\"].drop_duplicates())\n",
    "        mentions = mentions[mentions[\"ner_label\"].isin(ner_labels)]\n",
    "        if len(unknown_ner_labels) and unknown_labels == \"raise\":\n",
    "            raise Exception(f\"Unkown labels in {len_before-len(mentions)} mentions: \", unknown_ner_labels)\n",
    "    # Check that there is no mention overlap\n",
    "    if frag_merge:\n",
    "        mentions = mentions.merge(dataset[\"fragments\"].groupby([\"doc_id\", \"mention_id\"], as_index=False, observed=True).agg({\"begin\": \"min\", \"end\": \"max\"}))\n",
    "    print(\"Transform texts...\", end=\" \")\n",
    "    docs, deltas = apply_substitutions(\n",
    "        dataset[\"docs\"], *zip(\n",
    "            #(r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "            #(r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "            (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "            (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "        ), apply_unidecode=True)\n",
    "    docs = docs.astype({\"text\": str})\n",
    "    transformed_mentions = apply_deltas(mentions, deltas, on=['doc_id'])\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Splitting into sentences...\", end=\" \")\n",
    "    sentences = regex_sentencize(\n",
    "        docs, \n",
    "        reg_split=r\"(?<=[.])(\\s*\\n+)|(?=, [0-9]\\))\",\n",
    "        min_sentence_length=None, max_sentence_length=max_sentence_length,\n",
    "        balance_parentheses=False,\n",
    "        # balance_parentheses=True, # default is True\n",
    "    )\n",
    "    [mentions], sentences, sentence_to_docs = partition_spans([transformed_mentions], sentences, new_id_name=\"sentence_id\", overlap_policy=False)\n",
    "    \n",
    "    mentions = mentions[(mentions['begin']>0) & (mentions['end']>0)]\n",
    "    \n",
    "    n_sentences_per_mention = mentions.assign(count=1).groupby([\"doc_id\", \"mention_id\"], as_index=False).agg({\"count\": \"sum\", \"text\": tuple, \"sentence_id\": tuple})\n",
    "    if n_sentences_per_mention[\"count\"].max() > 1:\n",
    "        display(n_sentences_per_mention.query(\"count > 1\"))\n",
    "        display(sentences[sentences[\"sentence_id\"].isin(n_sentences_per_mention.query(\"count > 1\")[\"sentence_id\"].explode())][\"text\"].tolist())\n",
    "        raise Exception(\"Some mentions could be mapped to more than 1 sentences ({})\".format(n_sentences_per_mention[\"count\"].max()))\n",
    "    if sentence_to_docs is not None:\n",
    "        mentions = mentions.merge(sentence_to_docs)\n",
    "    mentions = mentions.assign(mention_idx=0).nlstruct.groupby_assign([\"doc_id\", \"sentence_id\"], {\"mention_idx\": lambda x: tuple(range(len(x)))})\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Tokenizing...\", end=\" \")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    sentences[\"text\"] = sentences[\"text\"].str.lower()\n",
    "    tokens = huggingface_tokenize(sentences, tokenizer, doc_id_col=\"sentence_id\")\n",
    "    \n",
    "    print(tokens)\n",
    "    \n",
    "    mentions = split_into_spans(mentions, tokens, pos_col=\"token_idx\", overlap_policy=False)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Processing nestings (overlapping areas)...\", end=\" \")\n",
    "    # Extract overlapping spans\n",
    "    conflicts = (\n",
    "        merge_with_spans(mentions, mentions, on=[\"doc_id\", \"sentence_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "    )\n",
    "    # ids1, and ids2 make the edges of the overlapping mentions of the same type (see the \"ner_label\")\n",
    "    [ids1, ids2], unique_ids = factorize_rows(\n",
    "        [conflicts[[\"doc_id\", \"sentence_id\", \"mention_id\"]], \n",
    "         conflicts[[\"doc_id\", \"sentence_id\", \"mention_id_other\"]]],\n",
    "        mentions.eval(\"size=(end-begin)\").sort_values(\"size\")[[\"doc_id\", \"sentence_id\", \"mention_id\"]]\n",
    "    )\n",
    "    g = nx.from_scipy_sparse_matrix(df_to_csr(ids1, ids2, n_rows=len(unique_ids), n_cols=len(unique_ids)))\n",
    "    colored_nodes = np.asarray(list(nx.coloring.greedy_color(g, strategy=keep_order).items()))\n",
    "    unique_ids['depth'] = colored_nodes[:, 1][colored_nodes[:, 0].argsort()]\n",
    "    mentions = mentions.merge(unique_ids)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Computing vocabularies...\")\n",
    "    [docs, sentences, mentions, tokens], vocs = normalize_vocabularies(\n",
    "        [docs, sentences, mentions, tokens], \n",
    "        vocabularies={\"split\": [\"train\", \"val\", \"test\"]} if vocabularies is None else vocabularies,\n",
    "        train_vocabularies={\"source\": False, \"text\": False} if vocabularies is None else False,\n",
    "        verbose=True)\n",
    "    print(\"done\")\n",
    "    \n",
    "    prep = Dataset(docs=docs, sentences=sentences, mentions=mentions, tokens=tokens).copy()\n",
    "    \n",
    "    unique_mention_ids = encode_ids([mentions], (\"doc_id\", \"sentence_id\", \"mention_id\"))\n",
    "    unique_sentence_ids = encode_ids([sentences, mentions, tokens], (\"doc_id\", \"sentence_id\"))\n",
    "    unique_doc_ids = encode_ids([docs, sentences, mentions, tokens], (\"doc_id\",))\n",
    "    \n",
    "    batcher = Batcher({\n",
    "        \"mention\": {\n",
    "            \"mention_id\": mentions[\"mention_id\"],\n",
    "            \"sentence_id\": mentions[\"sentence_id\"],\n",
    "            \"doc_id\": mentions[\"doc_id\"],\n",
    "            \"begin\": mentions[\"begin\"],\n",
    "            \"end\": mentions[\"end\"],\n",
    "            \"depth\": mentions[\"depth\"],\n",
    "            \"ner_label\": mentions[\"ner_label\"].cat.codes,\n",
    "        },\n",
    "        \"sentence\": {\n",
    "            \"sentence_id\": sentences[\"sentence_id\"],\n",
    "            \"doc_id\": sentences[\"doc_id\"],\n",
    "            \"mention_id\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], mentions[\"mention_id\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"mention_mask\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"token\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], tokens[\"token\"].cat.codes, n_rows=len(unique_sentence_ids)),\n",
    "            \"token_mask\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "        },\n",
    "        \"doc\": {\n",
    "            \"doc_id\": np.arange(len(unique_doc_ids)),\n",
    "            \"sentence_id\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], sentences[\"sentence_id\"], n_rows=len(unique_doc_ids)),\n",
    "            \"sentence_mask\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], n_rows=len(unique_doc_ids)),\n",
    "            \"split\": docs[\"split\"].cat.codes,\n",
    "        }},\n",
    "        masks={\"sentence\": {\"token\": \"token_mask\", \"mention_id\": \"mention_mask\"}, \n",
    "               \"doc\": {\"sentence_id\": \"sentence_mask\"}}\n",
    "    )\n",
    "    \n",
    "    return batcher, prep, deltas, vocs\n",
    "\n",
    "def keep_order(G, colors):\n",
    "    \"\"\"Returns a list of the nodes of ``G`` in ordered identically to their id in the graph\n",
    "    ``G`` is a NetworkX graph. ``colors`` is ignored.\n",
    "    This is to assign a depth using the nx.coloring.greedy_color function\n",
    "    \"\"\"\n",
    "    return sorted(list(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset(\n",
      "  (docs):       792 * ('doc_id', 'text', 'split')\n",
      "  (mentions):  7059 * ('doc_id', 'mention_id', 'category', 'text', 'begin', 'end', 'label_id', 'label')\n",
      "  (labels):    7059 * ('label_id', 'doc_id', 'mention_id', 'label')\n",
      "  (fragments): 6881 * ('doc_id', 'mention_id', 'begin', 'end', 'fragment_id')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... done\n",
      "Tokenizing... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/nlstruct/text/chunking/huggingface.py:11: FutureWarning: doc_id_col is not used anymore in the huggingface_tokenize function\n",
      "  warnings.warn(\"doc_id_col is not used anymore in the huggingface_tokenize function\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  token_id  token_idx    token  begin  end  sentence_idx    doc_id  split sentence_id\n",
      "0          0         0          0    [CLS]      0    0             0  10192393  train         0/0\n",
      "1          0         1          1        a      0    1             0  10192393  train         0/0\n",
      "2          0         2          2   common      2    8             0  10192393  train         0/0\n",
      "3          0         3          3    human      9   14             0  10192393  train         0/0\n",
      "4          0         4          4     skin     15   19             0  10192393  train         0/0\n",
      "...      ...       ...        ...      ...    ...  ...           ...       ...    ...         ...\n",
      "255920  2437    255920        163    ##pop    640  643             1   8696339    dev       791/1\n",
      "255921  2437    255921        164  ##tosis    643  648             1   8696339    dev       791/1\n",
      "255922  2437    255922        165        .    648  649             1   8696339    dev       791/1\n",
      "255923  2437    255923        166        .    649  650             1   8696339    dev       791/1\n",
      "255924  2437    255924        167    [SEP]    650  650             1   8696339    dev       791/1\n",
      "\n",
      "[255925 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlstruct:Will train vocabulary for category\n",
      "INFO:nlstruct:Will train vocabulary for ner_label\n",
      "INFO:nlstruct:Will train vocabulary for token\n",
      "INFO:nlstruct:Discovered existing vocabulary (28996 entities) for token\n",
      "INFO:nlstruct:Normalized split, with given vocabulary and no unk\n",
      "INFO:nlstruct:Normalized split, with given vocabulary and no unk\n",
      "INFO:nlstruct:Normalized split, with given vocabulary and no unk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Processing nestings (overlapping areas)... done\n",
      "Computing vocabularies...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-base-cased\"\n",
    "dataset = load_ncbi_disease()\n",
    "\n",
    "docs = dataset['docs']\n",
    "\n",
    "keep_n_first = None\n",
    "\n",
    "if keep_n_first:\n",
    "    docs = docs[:keep_n_first]\n",
    "    first_ids = docs['doc_id']\n",
    "    \n",
    "    first_mentions = dataset[\"mentions\"].loc[dataset[\"mentions\"]['doc_id'].isin(first_ids)]\n",
    "    first_fragments = dataset[\"fragments\"].loc[dataset[\"fragments\"]['doc_id'].isin(first_ids)]\n",
    "    first_attributes = dataset[\"attributes\"].loc[dataset[\"attributes\"]['doc_id'].isin(first_ids)]\n",
    "    \n",
    "    dataset[\"mentions\"] = first_mentions\n",
    "    dataset[\"fragments\"] = first_fragments\n",
    "    dataset[\"attributes\"] = first_attributes\n",
    "    \n",
    "dataset['mentions'] = dataset['mentions'].merge(dataset[\"fragments\"].groupby([\"doc_id\", \"mention_id\"], as_index=False, observed=True).agg({\"begin\": \"min\", \"end\": \"max\"}))\n",
    "\n",
    "merged = dataset['mentions'].merge(dataset['labels'], on=['doc_id', 'mention_id'])\n",
    "merged['mention_id'] = merged['label_id']\n",
    "merged['label'] = merged['category']\n",
    "    \n",
    "dataset['docs'] = docs\n",
    "\n",
    "dataset['mentions'] = merged\n",
    "\n",
    "ner_labels = list(dataset['mentions']['label'].unique())\n",
    "\n",
    "batcher, prep, deltas, vocs = preprocess_train(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=140,\n",
    "    bert_name=bert_name,\n",
    "    ner_labels= ner_labels,\n",
    "    unknown_labels=\"drop\",\n",
    "    frag_merge=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "bert, log = BertModel.from_pretrained(bert_name, output_loading_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (L2 dist) between train and val frequencies: 0.0013780224108268146\n",
      "Frequencies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CompositeMention</th>\n",
       "      <th>DiseaseClass</th>\n",
       "      <th>Modifier</th>\n",
       "      <th>SpecificDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.035844</td>\n",
       "      <td>0.155054</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.549335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>0.036442</td>\n",
       "      <td>0.126474</td>\n",
       "      <td>0.282958</td>\n",
       "      <td>0.554126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  CompositeMention  DiseaseClass  Modifier  SpecificDisease\n",
       "0  train          0.035844      0.155054  0.259766         0.549335\n",
       "1    val          0.036442      0.126474  0.282958         0.554126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#all_test_doc_ids = []\n",
    "#sims = {}\n",
    "#for i in range(200):\n",
    "seed_all(1234567+137)\n",
    "\n",
    "train_batcher = batcher['doc'][batcher['doc']['split']==0]['sentence']\n",
    "test_batcher = batcher['doc'][batcher['doc']['split']==2]['sentence']\n",
    "\n",
    "splits = np.zeros(len(train_batcher['doc']), dtype=int)\n",
    "\n",
    "val_perc = 0.1\n",
    "splits[np.random.choice(np.arange(len(splits)), size=int(val_perc * len(splits)))] = 1\n",
    "\n",
    "val_batcher = test_batcher#batcher['sentence'][splits == 1]\n",
    "\n",
    "# train_val_split = np.random.permutation(len(train_batcher))\n",
    "# test_batcher = train_batcher[train_val_split[:int(0.1*len(train_val_split))]]['sentence']\n",
    "# train_batcher = train_batcher[train_val_split[int(0.1*len(train_val_split)):]]['sentence']\n",
    "sim = ((np.bincount(val_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(val_batcher['mention']) -\n",
    "np.bincount(train_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(train_batcher['mention']))**2).sum()\n",
    "print(\"Similarity (L2 dist) between train and val frequencies:\", sim)\n",
    "print(\"Frequencies\")\n",
    "#all_test_doc_ids.append((test_doc_ids, sim))\n",
    "display(pd.DataFrame([\n",
    "    {\"index\": \"train\", **dict(zip(vocs[\"ner_label\"], np.bincount(train_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(train_batcher['mention'])))},\n",
    "    {\"index\": \"val\", **dict(zip(vocs[\"ner_label\"], np.bincount(val_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(val_batcher['mention'])))},\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def torch_f1(actions, target_tags, is_training=True):\n",
    "    f1 = torch.from_numpy(np.array((f1_score(target_tags.reshape(-1).cpu(), actions.reshape(-1).cpu(), average=\"micro\"))))\n",
    "    f1.requires_grad = is_training\n",
    "    return f1\n",
    "\n",
    "class NERNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels,\n",
    "                 hidden_dim,\n",
    "                 dropout,\n",
    "                 n_tokens=None,\n",
    "                 token_dim=None,\n",
    "                 embeddings=None,\n",
    "                 tag_scheme=\"bio\",\n",
    "                 max_depth=10,\n",
    "                 lstm_size=100,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embeddings = embeddings\n",
    "            if n_tokens is None or token_dim is None:\n",
    "                if hasattr(embeddings, 'weight'):\n",
    "                    n_tokens, token_dim = embeddings.weight.shape\n",
    "                else:\n",
    "                    n_tokens, token_dim = embeddings.embeddings.weight.shape\n",
    "        else:\n",
    "            self.embeddings = torch.nn.Embedding(n_tokens, token_dim) if n_tokens > 0 else None\n",
    "        assert token_dim is not None, \"Provide token_dim or embeddings\"\n",
    "        assert self.embeddings is not None\n",
    "\n",
    "        dim = (token_dim if n_tokens > 0 else 0)\n",
    "        \n",
    "        self.ner_labels_subset = list(range(n_labels))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        if tag_scheme == \"bio\":\n",
    "            self.crf_list = torch.nn.ModuleList([BIODecoder(1, with_start_end_transitions=False) for _ in self.ner_labels_subset])\n",
    "        elif tag_scheme == \"bioul\":\n",
    "            self.crf_list = torch.nn.ModuleList([BIOULDecoder(1, with_start_end_transitions=False) for _ in self.ner_labels_subset])\n",
    "        else:\n",
    "            raise Exception()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        self.linear = torch.nn.Linear(dim, hidden_dim)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        n_tags = self.crf_list[0].num_tags\n",
    "        \n",
    "        self.metric_fc = torch.nn.Linear(dim, sum(crf.num_tags for crf in self.crf_list))\n",
    "        \n",
    "    def forward(self, \n",
    "                tokens, \n",
    "                mask,\n",
    "                tags=None,\n",
    "                return_loss=False,):\n",
    "        # Embed the tokens\n",
    "        scores = None\n",
    "        # shape: n_batch * sequence * 768\n",
    "        embeds = self.embeddings(tokens)[0]\n",
    "        \n",
    "        state = embeds.masked_fill(~mask.unsqueeze(-1), 0)\n",
    "        state = torch.relu(state)#self.linear(self.dropout(state)))# + state\n",
    "        state = self.batch_norm(state.view(-1, state.shape[-1])).view(state.shape)\n",
    "        \n",
    "        scores = self.metric_fc(state)\n",
    "        \n",
    "        scores = scores.reshape((*scores.shape[:-1], -1, len(self.crf_list))).permute(3, 0, 1, 2)\n",
    "        \n",
    "        sampled_spans = []\n",
    "        sampled_tags = []\n",
    "        baseline_tags = []\n",
    "            \n",
    "        for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list)):\n",
    "            sample_tags = crf.sample(scores[i], mask, n=1)\n",
    "            argmax_tags = crf.decode(scores[i], mask)\n",
    "            extracted = crf.tags_to_spans(sample_tags[0], mask)\n",
    "            \n",
    "            extracted['span_label'] = torch.full_like(extracted[\"span_label\"], ner_label_idx)\n",
    "        \n",
    "            sampled_spans.append(extracted)\n",
    "            sampled_tags.append(sample_tags)\n",
    "            baseline_tags.append(argmax_tags)\n",
    "        \n",
    "        if return_loss:\n",
    "            loss = -torch.stack([crf(scores[i], mask, tags[0][..., ner_label_idx], reduction=\"none\") \n",
    "                                for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list))]).mean()\n",
    "            \n",
    "        if len(sampled_spans)>0:\n",
    "            sampled_spans = {\n",
    "                k: torch.cat([sm[k] for sm in sampled_spans], -1) for k in sampled_spans[0].keys() \n",
    "            }\n",
    "    \n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"sampled_spans\": sampled_spans,\n",
    "            \"loss\": loss if return_loss else None,\n",
    "            \"sampled_tags\": sampled_tags,\n",
    "            \"baseline_tags\": sampled_tags,\n",
    "        }\n",
    "    \n",
    "    def init_lstm_state(self, seq_len):\n",
    "        return (torch.autograd.Variable(torch.zeros(1, seq_len, self.lstm_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, seq_len, self.lstm_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(nn.Module):\n",
    "    \"\"\"An abstract attention module.\n",
    "\n",
    "    All attention classes inherit from this class.\n",
    "    Attention refers to focusing on the context based\n",
    "    on a (usually learnt) key.\n",
    "\n",
    "    Arguments:\n",
    "        input_emb_size (int): The ctxt embedding size.\n",
    "        key_emb_size (int): The size of the key\n",
    "        output_emb_size (int): The output embedding size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_emb_size: int,\n",
    "                 key_emb_size: int,\n",
    "                 output_emb_size: int):\n",
    "        super(BaseAttention, self).__init__()\n",
    "        self.input_emb_size = input_emb_size\n",
    "        self.key_emb_size = key_emb_size\n",
    "        self.output_emb_size = output_emb_size\n",
    "\n",
    "\n",
    "        print(\"INIT BaseAttention \")\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"This is an abstract class\")\n",
    "\n",
    "    def get_output_dim(self):\n",
    "        \"\"\"The output embedding size\n",
    "        \"\"\"\n",
    "        return self.output_emb_size\n",
    "\n",
    "    def get_key_dim(self):\n",
    "        \"\"\"The key embedding size\n",
    "        \"\"\"\n",
    "        return self.key_emb_size\n",
    "\n",
    "    def get_input_dim(self):\n",
    "        \"\"\"The input embedding size\n",
    "        \"\"\"\n",
    "        return self.input_emb_size\n",
    "\n",
    "\n",
    "class KeyedAttention(BaseAttention):\n",
    "    \"\"\"Computes a single attention distribution,\n",
    "    based on a learned key.\n",
    "\n",
    "    Arguments:\n",
    "        key_dim (int): The size of the key\n",
    "        ctxt_dim (int): The size of the context\n",
    "        attn_type (str): (\"sum\"|\"dot\") The type of\n",
    "            attention mechanism. \"sum\" is the usual\n",
    "            Bahdanau model of attention, while\n",
    "            \"dot\" is the dot product\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key_dim, ctxt_dim, attn_type, dropout=0.0,\n",
    "                 temperature=1.):\n",
    "        super(KeyedAttention, self).__init__(\n",
    "            input_emb_size=ctxt_dim,\n",
    "            key_emb_size=key_dim,\n",
    "            output_emb_size=ctxt_dim\n",
    "        )\n",
    "        self.attn_type = attn_type\n",
    "        self.proj_ctxt = nn.Linear(ctxt_dim, key_dim)\n",
    "        self.key = nn.Parameter(torch.Tensor(key_dim, 1).uniform_(-0.01, 0.01))\n",
    "        self._dropout = nn.Dropout(p=dropout) if dropout != 0. else None\n",
    "        self.temperature = temperature\n",
    "        if self.attn_type == \"sum\":\n",
    "            self.proj_ctxt_key_matrix = nn.Linear(key_dim, 1)\n",
    "\n",
    "        print(\"INIT KEYED ATTENTION\")\n",
    "\n",
    "    def forward(self, context, mask):\n",
    "        \"\"\"The forward pass\n",
    "\n",
    "        Arguments:\n",
    "            context (``torch.Tensor``):\n",
    "                batch x seq_len x embed_dim: The Context\n",
    "            mask (``torch.LongTensor``):\n",
    "                batch x seq_len: The context mask\n",
    "        Returns:\n",
    "            (``torch.Tensor``, ``torch.Tensor``)\n",
    "\n",
    "            weighed_emb: batch x output_embed_size:\n",
    "            The attention weighted embeddings\n",
    "\n",
    "            attn_weights: batch x seq_len\n",
    "            The attention weights\n",
    "\n",
    "        \"\"\"\n",
    "        proj_ctxt = self.proj_ctxt(context)  # batch x seq_len x key_dim\n",
    "        if self.attn_type == \"dot\":\n",
    "            batch, seq_len, key_dim = proj_ctxt.size()\n",
    "            scores = torch.mm(proj_ctxt.view(-1, key_dim), self.key)\n",
    "            logits = scores.contiguous().view(batch, seq_len)\n",
    "        elif self.attn_type == \"sum\":\n",
    "            batch, seq_len, key_dim = proj_ctxt.size()\n",
    "            expanded_key = self.key.transpose(0, 1).expand(batch, seq_len, -1)\n",
    "            ctxt_key_matrix = torch.tanh(expanded_key + proj_ctxt)\n",
    "            logits = self.proj_ctxt_key_matrix(ctxt_key_matrix).squeeze(-1)\n",
    "        logits /= self.temperature\n",
    "        if mask is not None:\n",
    "            float_mask = mask.float()\n",
    "            negval = -10e5\n",
    "            logits = (float_mask * logits) + ((1 - float_mask) * negval)\n",
    "        attn_weights = F.softmax(logits, -1).unsqueeze(1)\n",
    "        if self._dropout is not None:\n",
    "            attn_weights = self._dropout(attn_weights)\n",
    "        weighted_emb = torch.bmm(attn_weights, context).squeeze(1)\n",
    "\n",
    "        return weighted_emb, attn_weights.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassifier(Model):\n",
    "    \"\"\"This class is similar to the previous one, except that\n",
    "    it handles multi level classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Vocabulary,\n",
    "        method: str,\n",
    "        text_field_embedder: TextFieldEmbedder,\n",
    "        encoder_word: Seq2SeqEncoder,\n",
    "        attn_word: List[Attns.BaseAttention],\n",
    "        label_indexer: LabelIndexer,\n",
    "        thresh: float = 0.5,\n",
    "        initializer: InitializerApplicator = InitializerApplicator(),\n",
    "        regularizer: Optional[RegularizerApplicator] = None\n",
    "    ) -> 'MultiClassifier':\n",
    "        super(MultiClassifier, self).__init__(vocab, regularizer)\n",
    "        # Label info\n",
    "        self.label_indexer = label_indexer\n",
    "        self.num_labels = self.label_indexer.get_num_tags()\n",
    "        self.method = method\n",
    "\n",
    "        # Prediction thresholds\n",
    "        self.thresh = thresh\n",
    "        self.log_thresh = np.log(thresh + 1e-5)\n",
    "        # Model\n",
    "        # Text encoders\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        # Sentence and doc encoders\n",
    "        self.encoder_word = encoder_word\n",
    "        # Attention Modules\n",
    "        # We use setattr, so that cuda properties translate.\n",
    "        # Otherwise, it becomes a little messy\n",
    "        for ix in range(self.num_labels - 1):\n",
    "            tag = self.label_indexer.get_tag(ix)\n",
    "            setattr(self, f\"attn_{tag}\", attn_word[ix])\n",
    "        # Label prediction\n",
    "        self.output_dim = attn_word[0].get_output_dim()\n",
    "        if self.method == \"binary\":\n",
    "            for ix in range(self.num_labels):\n",
    "                tag = self.label_indexer.get_tag(ix)\n",
    "                module = Linear(self.output_dim, 1)\n",
    "                setattr(self, f\"logits_layer_{tag}\", module)\n",
    "        elif self.method == \"softmax\":\n",
    "            module = Linear(\n",
    "                self.output_dim * (self.num_labels - 1), self.num_labels)\n",
    "            setattr(self, \"logits_layer\", module)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "        self.classification_metric = ClassificationMetrics(\n",
    "            label_indexer)\n",
    "        # self.classification_metric = BooleanAccuracy()\n",
    "        initializer(self)\n",
    "\n",
    "        # Some dimension checks\n",
    "        check_dimensions_match(\n",
    "            text_field_embedder.get_output_dim(), encoder_word.get_input_dim(),\n",
    "            \"text field embedding dim\", \"word encoder input dim\")\n",
    "        check_dimensions_match(\n",
    "            encoder_word.get_output_dim(), attn_word[0].get_input_dim(),\n",
    "            \"word encoder output\", \"word attention input\")\n",
    "\n",
    "\n",
    "        print(\"INIT MULTICLASSIFIER\")\n",
    "\n",
    "    @overrides\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: Dict[str, LongTensor],\n",
    "        labels: LongTensor = None,\n",
    "        tags: LongTensor = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        \"\"\"The forward pass\n",
    "\n",
    "        Commonly used symbols:\n",
    "            S : Max sent length\n",
    "            C : Max word length\n",
    "            L : Number of tags (Including the O tag)\n",
    "\n",
    "        Arguments:\n",
    "            tokens (Dict[str, ``LongTensor``]): The indexed values\n",
    "                Contains the following:\n",
    "                    * tokens: batch_size x S\n",
    "                    * chars: batch_size x S x C\n",
    "                    * elmo [Optional]: batch_size x S x C\n",
    "\n",
    "            labels (``LongTensor``) : batch x L: The labels\n",
    "            tags (``LongTensor``) : batch x S : The gold NER tags\n",
    "\n",
    "        ..note::\n",
    "            Need to incorporate pos_tags etc. into kwargs\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, ``LongTensor``]: A dictionary with the following\n",
    "            attributes\n",
    "\n",
    "                * loss: 1 x 1 : The BCE Loss\n",
    "                * logits: (batch, num_tags) : The output of the logits\n",
    "                    for class prediction\n",
    "                * log_probs: (batch, num_tags) : The output\n",
    "                    for class prediction\n",
    "                * attentions: List[batch x S]:\n",
    "                  The attention over each word in the sentence,\n",
    "                  for each tag\n",
    "                * preds: (batch, num_tags) : The probabilites predicted\n",
    "        \"\"\"\n",
    "        if len(kwargs) > 0:\n",
    "            raise NotImplementedError(\"Don't handle features yet\")\n",
    "        emb_msg = self.text_field_embedder(tokens)\n",
    "        mask = util.get_text_field_mask(tokens)  # num_sents x S\n",
    "        encoded_msg = self.encoder_word(emb_msg, mask)\n",
    "        attentions = []\n",
    "        sent_embs = []\n",
    "        for ix in range(self.num_labels - 1):\n",
    "            tag = self.label_indexer.get_tag(ix)\n",
    "            sent_emb, sent_attn = getattr(self, f\"attn_{tag}\")(\n",
    "                encoded_msg, mask)\n",
    "            '''\n",
    "                sent_emb: batch x embed_dim\n",
    "                sent_attns: batch x T\n",
    "            '''\n",
    "            sent_embs.append(sent_emb)\n",
    "            attentions.append(sent_attn.unsqueeze(-1))\n",
    "\n",
    "        attentions = torch.cat(attentions, -1)\n",
    "\n",
    "        outputs = {\n",
    "            \"attentions\": attentions,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "\n",
    "        smoothing = 0.1\n",
    "        if self.method == \"binary\":\n",
    "            all_logits = []\n",
    "            for ix in range(self.num_labels - 1):\n",
    "                tag = self.label_indexer.get_tag(ix)\n",
    "                sent_emb = sent_embs[ix]\n",
    "                logits = getattr(self, f\"logits_layer_{tag}\")(sent_emb)\n",
    "                all_logits.append(logits)\n",
    "            all_logits = torch.cat(all_logits, -1)\n",
    "            log_probs = F.logsigmoid(all_logits)\n",
    "            outputs[\"logits\"] = all_logits\n",
    "            outputs[\"log_probs\"] = log_probs\n",
    "            pred_labels = log_probs.gt(self.log_thresh).long()\n",
    "            outputs[\"preds\"] = pred_labels\n",
    "            if labels is not None:\n",
    "                labels = labels[:, :-1]\n",
    "                soft_labels = labels + smoothing - (2 * smoothing * labels)\n",
    "                loss = -(soft_labels * log_probs +\n",
    "                         ((1 - soft_labels) * F.logsigmoid(-all_logits)))\n",
    "                loss = loss.mean(-1).mean()\n",
    "                outputs[\"loss\"] = loss\n",
    "                self.classification_metric(pred_labels.long(), labels.long())\n",
    "                self.decode(outputs)\n",
    "        else:\n",
    "            # softmax method\n",
    "            sent_emb = torch.cat([sent_embs], -1)\n",
    "            all_logits = self.logits_layer(sent_emb)\n",
    "            raise NotImplementedError(\"Work in progress\")\n",
    "        return outputs\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metric_dict = self.classification_metric.get_metric(\n",
    "            reset=reset)\n",
    "        # return OrderedDict({x: y for x, y in metric_dict.items()})\n",
    "        return metric_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, outputs):\n",
    "        \"\"\"\n",
    "        This decodes the outputs of the model into a format used downstream\n",
    "        for predictions\n",
    "\n",
    "        Arguments:\n",
    "            outputs (List[Dict]) : The outputs generated by the model\n",
    "                Must contain\n",
    "                    * mask : The mask for the current batch\n",
    "                    * preds (batch x num_tags - 1) : The predictions\n",
    "                        note that if nothing is predicted, we predict \"O\"\n",
    "                    * attentions (batch x seq_len x num_tags) : The attentions\n",
    "\n",
    "        Returns:\n",
    "            decoded_output (Dict) : The decoded output\n",
    "                Must contain:\n",
    "                    * preds (List[List[str]]) : The predicted tags\n",
    "                    * attentions (List[Dict]) : List of dictionaries\n",
    "                        mapping each tag to its attention distribution\n",
    "        \"\"\"\n",
    "        decoded_output = {\n",
    "            \"preds\": [],\n",
    "            \"attentions\": []\n",
    "        }\n",
    "        lengths = outputs[\"mask\"].sum(-1)\n",
    "        lengths = to_numpy(lengths, lengths.is_cuda)\n",
    "        predictions = to_numpy(outputs[\"preds\"], outputs[\"preds\"].is_cuda)\n",
    "        log_probs = to_numpy(\n",
    "            outputs[\"log_probs\"], outputs[\"log_probs\"].is_cuda)\n",
    "        attentions = to_numpy(\n",
    "            outputs[\"attentions\"], outputs[\"attentions\"].is_cuda)\n",
    "        for ix in range(lengths.size):\n",
    "            non_zero_indices = np.nonzero(predictions[ix])[0]\n",
    "            pred_list = []\n",
    "            for kx in range(non_zero_indices.shape[0]):\n",
    "                pred_list.append(\n",
    "                    [\n",
    "                        self.label_indexer.get_tag(\n",
    "                            non_zero_indices[kx]\n",
    "                        ),\n",
    "                        np.exp(log_probs[ix, non_zero_indices[kx]])\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            if len(pred_list) == 0:\n",
    "                pred_list.append([\"O\", 1.0])\n",
    "            decoded_output[\"preds\"].append(pred_list)\n",
    "            attention = OrderedDict()\n",
    "            for jx in range(attentions[ix].shape[-1]):\n",
    "                tag = self.label_indexer.get_tag(jx)\n",
    "                attention[tag] = attentions[ix, :lengths[ix], jx].tolist()\n",
    "            decoded_output[\"attentions\"].append(attention)\n",
    "        return decoded_output\n",
    "\n",
    "    @classmethod\n",
    "    @overrides\n",
    "    def from_params(\n",
    "        cls,\n",
    "        vocab: Vocabulary,\n",
    "        params: Params,\n",
    "        label_indexer: LabelIndexer\n",
    "    ) -> 'MultiClassifer':\n",
    "        method = params.pop(\"method\", \"binary\")\n",
    "        num_tags = label_indexer.get_num_tags()\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(\n",
    "            vocab, embedder_params\n",
    "        )\n",
    "        encoder_word = Seq2SeqEncoder.from_params(params.pop(\"encoder_word\"))\n",
    "        attn_word_params = params.pop(\"attention_word\")\n",
    "        attn_type = attn_word_params.pop(\"type\")\n",
    "\n",
    "        attn_word = []\n",
    "        for ix in range(num_tags - 1):\n",
    "            # Since from_params clears out the dictionaries,\n",
    "            # this deepcopy is necessary\n",
    "            tmp_attn_params = deepcopy(attn_word_params)\n",
    "            attn_word.append(\n",
    "                getattr(Attns, attn_type).from_params(tmp_attn_params))\n",
    "\n",
    "        threshold = params.pop(\"threshold\", 0.5)\n",
    "        initializer = InitializerApplicator.from_params(\n",
    "            params.pop('initializer', [])\n",
    "        )\n",
    "        regularizer = RegularizerApplicator.from_params(\n",
    "            params.pop('regularizer', [])\n",
    "        )\n",
    "        return cls(\n",
    "            vocab=vocab,\n",
    "            method=method,\n",
    "            text_field_embedder=text_field_embedder,\n",
    "            encoder_word=encoder_word,\n",
    "            attn_word=attn_word,\n",
    "            thresh=threshold,\n",
    "            initializer=initializer,\n",
    "            regularizer=regularizer,\n",
    "            label_indexer=label_indexer\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionSegmentation.model.classifiers import MultiClassifier\n",
    "\n",
    "model = MultiClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlstruct:Available CUDA devices: 1\n",
      "INFO:nlstruct:Current device: cuda:0\n",
      "100%|██████████| 57/57 [00:08<00:00,  6.43it/s, loss=0.364]\n",
      "INFO:nlstruct:epoch | train_ner_loss | \u001b[31mval_f1\u001b[0m |    dur(s)\n",
      "INFO:nlstruct:    1 |         \u001b[32m0.4266\u001b[0m | \u001b[32m0.0000\u001b[0m |    8.8676\n",
      "100%|██████████| 57/57 [00:08<00:00,  6.48it/s, loss=0.434]\n",
      "INFO:nlstruct:    2 |         \u001b[32m0.3798\u001b[0m | \u001b[31m0.0000\u001b[0m |    8.7995\n",
      "100%|██████████| 57/57 [00:08<00:00,  6.47it/s, loss=0.264]\n",
      "INFO:nlstruct:    3 |         \u001b[32m0.3784\u001b[0m | \u001b[31m0.0000\u001b[0m |    8.8194\n",
      "100%|██████████| 57/57 [00:08<00:00,  6.44it/s, loss=0.442]\n",
      "INFO:nlstruct:    4 |         \u001b[32m0.3733\u001b[0m | \u001b[31m0.0000\u001b[0m |    8.8604\n",
      " 77%|███████▋  | 44/57 [00:07<00:02,  6.26it/s, loss=0.298]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2d5f56237253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Perform optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_nets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0msentence_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def select_mention_level(batch_mentions, depth_level):\n",
    "    tag_mask = batch_mentions[\"mention\", \"depth\"]==depth_level\n",
    "    return batch_mentions[\"mention\"][tag_mask]\n",
    "\n",
    "def labels_from_batch(batch):\n",
    "    unique_sent_labels = pd.DataFrame(\n",
    "        {'sentence_id':batch['mention']['sentence_id'].cpu(), 'ner_label':batch['mention']['ner_label'].cpu()}\n",
    "    )\n",
    "\n",
    "    unique_sent_labels = unique_sent_labels.assign(sentence_idx=(unique_sent_labels['sentence_id']).astype('category').cat.codes)\n",
    "\n",
    "    sentence_labels = torch.zeros((len(batch), len(ner_labels) + 1)).to(batch.device)\n",
    "\n",
    "    sentence_labels[unique_sent_labels['sentence_idx'],unique_sent_labels['ner_label']] = 1\n",
    "    \n",
    "    return sentence_labels\n",
    "\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AdamW, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from nlstruct.environment import get_cache\n",
    "from nlstruct.utils import evaluating, torch_global as tg, freeze\n",
    "from nlstruct.scoring import compute_metrics, merge_pred_and_gold\n",
    "from nlstruct.train import make_optimizer_and_schedules, iter_optimization, seed_all\n",
    "from nlstruct.train.schedule import ScaleOnPlateauSchedule, LinearSchedule, ConstantSchedule\n",
    "\n",
    "from nlstruct.utils import torch_clone\n",
    "    \n",
    "device = torch.device('cuda:0')\n",
    "tg.set_device(device)\n",
    "all_preds = []\n",
    "histories = []\n",
    "\n",
    "# To release gpu memory before allocating new parameters for a new model\n",
    "# A better idea would be to run xp in a function, so that all variables are released when exiting the fn\n",
    "# but this way we can debug after this cell if something goes wrong\n",
    "if \"all_nets\" in globals(): del all_nets\n",
    "if \"optim\" in globals(): del optim, \n",
    "if \"schedules\" in globals(): del schedules\n",
    "if \"final_schedule\" in globals(): del final_schedule\n",
    "if \"state\" in globals(): del state\n",
    "    \n",
    "# Hyperparameter search\n",
    "layer = 13\n",
    "hidden_dim = 256 \n",
    "scheme = 'bioul' \n",
    "seed = 12\n",
    "lr = 7e-4\n",
    "bert_lr = 6e-5\n",
    "dropout = 0.1\n",
    "\n",
    "seed_all(seed) # /!\\ Super important to enable reproducibility\n",
    "\n",
    "max_grad_norm = 5.\n",
    "bert_weight_decay = 0.0000\n",
    "batch_size = 32\n",
    "random_perm=True\n",
    "n_freeze = layer + 2\n",
    "bert_dropout = 0.2\n",
    "\n",
    "sentence_classifier = SentenceClassifier(\n",
    "        n_tokens=len(vocs[\"token\"]),\n",
    "        token_dim=1024 if \"large\" in bert_name else 768,#768,\n",
    "        embeddings=BertModel.from_pretrained(bert_name),#, custom_embeds_layer_index=custom_embeds_layer_index),\n",
    "        n_labels = len(vocs['ner_label']) + 1,\n",
    "        dropout=dropout,\n",
    "        hidden_dim=hidden_dim,\n",
    ")\n",
    "\n",
    "ner_net = NERNet(\n",
    "        n_tokens=len(vocs[\"token\"]),\n",
    "        token_dim=1024 if \"large\" in bert_name else 768,#768,\n",
    "        embeddings=BertModel.from_pretrained(bert_name),#, custom_embeds_layer_index=custom_embeds_layer_index),\n",
    "        n_labels = len(vocs['ner_label']),\n",
    "        dropout=dropout,\n",
    "        hidden_dim=hidden_dim,\n",
    "        tag_scheme=scheme,\n",
    ")\n",
    "\n",
    "all_nets = torch.nn.ModuleDict({\n",
    "    \"sentence_classifier\": sentence_classifier,\n",
    "    \"ner_net\": ner_net,\n",
    "}).to(device=tg.device)\n",
    "\n",
    "del ner_net\n",
    "\n",
    "for module in all_nets[\"sentence_classifier\"].embeddings.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = bert_dropout\n",
    "all_nets.train()\n",
    "\n",
    "for module in all_nets[\"ner_net\"].embeddings.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = bert_dropout\n",
    "all_nets.train()\n",
    "\n",
    "# Define the optimizer, maybe multiple learning rate / schedules per parameters groups\n",
    "sentence_optim = AdamW(params=all_nets['sentence_classifier'].parameters(), lr=lr)\n",
    "optim = AdamW(params=all_nets['ner_net'].parameters(), lr=lr)\n",
    "\n",
    "# Freeze some bert layers \n",
    "# - n_freeze = 0 to freeze nothing\n",
    "# - n_freeze = 1 to freeze word embeddings / position embeddings / ...\n",
    "# - n_freeze = 2..13 to freeze the first, second ... 12th layer of bert\n",
    "for name, param in all_nets.named_parameters():\n",
    "    match = re.search(\"\\.(\\d+)\\.\", name)\n",
    "    if (match and int(match.group(1)) < n_freeze - 1) and ('embeddings' in name):\n",
    "        freeze([param])\n",
    "        \n",
    "with_tqdm = True\n",
    "state = {\"all_nets\": all_nets, \"optim\": optim, \"sentence_optim\": sentence_optim}  # all we need to restart the training from a given epoch\n",
    "\n",
    "cache = get_cache(\"ncbi_ws_tests\", {\n",
    "    \"seed\": seed, \n",
    "    \"train_batcher\": train_batcher, \n",
    "    \"val_batcher\": None, \n",
    "    \"random_perm\": random_perm,\n",
    "    \"batch_size\": batch_size, \n",
    "    \"max_grad_norm\": max_grad_norm, \n",
    "    **state,\n",
    "}, loader=torch.load, dumper=torch.save)\n",
    "\n",
    "!rm -rf $cache\n",
    "\n",
    "cache = get_cache(\"ncbi_ws_tests\", {\n",
    "    \"seed\": seed, \n",
    "    \"train_batcher\": train_batcher, \n",
    "    \"val_batcher\": None, \n",
    "    \"random_perm\": random_perm,\n",
    "    \"batch_size\": batch_size, \n",
    "    \"max_grad_norm\": max_grad_norm, \n",
    "    **state\n",
    "}, loader=torch.load, dumper=torch.save)\n",
    "\n",
    "level = 0\n",
    "\n",
    "level_train_batcher = train_batcher['mention'].set_join_order(('mention',))[train_batcher['mention','depth']==level]\n",
    "level_val_batcher = val_batcher['mention'].set_join_order(('mention',))[val_batcher['mention','depth']==level]\n",
    "\n",
    "CELoss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "for epoch_before, state, history, record in iter_optimization(\n",
    "    main_score = \"val_f1\", # do not earlystop based on validation\n",
    "    metrics_info=metrics_info,\n",
    "    max_epoch=50,\n",
    "    patience=50,\n",
    "    state=state, \n",
    "    cache_policy=\"all\", \n",
    "    #cache=cache,\n",
    "    n_save_checkpoints=2,\n",
    "#             exit_on_score=0.92,\n",
    "):\n",
    "    pred_batches = []\n",
    "    gold_batches = []\n",
    "\n",
    "    total_train_ner_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_ner_size = 0\n",
    "\n",
    "    n_mentions = len(train_batcher[\"mention\"])\n",
    "    n_matched_mentions = 0\n",
    "    n_target_mentions = 0\n",
    "    n_observed_mentions = 0\n",
    "\n",
    "    #with tqdm(repeat(batch, 1000), disable=not with_tqdm) as bar:\n",
    "    with tqdm(train_batcher['sentence'].dataloader(batch_size=batch_size, shuffle=True, sparse_sort_on=[\"token_mask\"], device=device), disable=not with_tqdm) as bar:\n",
    "        for batch_i, batch in enumerate(bar):\n",
    "            sentence_optim.zero_grad()\n",
    "            \n",
    "            mask = batch[\"token_mask\"]\n",
    "            \n",
    "            sentence_labels = labels_from_batch(batch)\n",
    "            \n",
    "            res = all_nets[\"sentence_classifier\"].forward(\n",
    "                batch[\"token\"],\n",
    "#                 tokens=batch[\"token\"],\n",
    "#                 mask=mask,\n",
    "            )\n",
    "            \n",
    "            scores = res[\"scores\"]\n",
    "            attention = res[\"attention\"]\n",
    "            \n",
    "            loss = CELoss(sigmoid(scores), sentence_labels)\n",
    "                \n",
    "            total_train_ner_loss += float(loss) * len(batch[\"sentence\"])\n",
    "            total_train_ner_size += len(batch[\"sentence\"])\n",
    "                \n",
    "            # Perform optimization step\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(all_nets.parameters(), max_grad_norm)\n",
    "            sentence_optim.step()\n",
    "            \n",
    "            bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "    train_metrics = {}\n",
    "    val_metrics = {'val_f1': 0}\n",
    "\n",
    "    record(\n",
    "    {\n",
    "        \"train_ner_loss\": total_train_ner_loss / max(total_train_ner_size, 1),\n",
    "        **train_metrics,\n",
    "        **val_metrics,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             optim.zero_grad()\n",
    "\n",
    "#             n_samples, sentence_size = batch[\"sentence\", \"token\"].shape\n",
    "#             n_labels = len(vocs[\"ner_label\"])\n",
    "#             n_depth = batch[\"mention\", \"depth\"].max() + 1\n",
    "            \n",
    "#             mask = batch[\"token_mask\"]\n",
    "\n",
    "#             # Compute the tokens label tag of the selected non-overlapping gold mentions to infer from the model\n",
    "\n",
    "#             target_tags = BIOULDecoder.spans_to_tags(\n",
    "#                 batch[\"mention\", \"depth\"] * n_labels * n_samples + (batch[\"mention\", \"@sentence_id\"] * n_labels + batch[\"mention\", \"ner_label\"]),\n",
    "#                 batch[\"mention\", \"begin\"],\n",
    "#                 batch[\"mention\", \"end\"], \n",
    "#                 torch.zeros_like(batch[\"mention\", \"ner_label\"]),\n",
    "#                 n_tokens=sentence_size,\n",
    "#                 n_samples=n_samples * n_labels * n_depth,\n",
    "#             )\n",
    "#             target_tags = target_tags.view(n_depth, n_samples, n_labels, sentence_size)\n",
    "#             target_tags = target_tags.transpose(-1, -2)\n",
    "            \n",
    "\n",
    "#             res = all_nets[\"ner_net\"].forward(\n",
    "#                 tokens=batch[\"token\"],\n",
    "#                 mask=mask,\n",
    "#                 tags=target_tags,\n",
    "#                 return_loss = True,\n",
    "#             )\n",
    "            \n",
    "#             ner_loss = res['loss']\n",
    "#             scores = res['scores']\n",
    "#             spans = res['sampled_spans']\n",
    "#             sampled_tags = res['sampled_tags']\n",
    "#             baseline_tags = res['baseline_tags']\n",
    "            \n",
    "#             pred_batch = Batcher({\n",
    "#                 \"mention\": {\n",
    "#                     \"mention_id\": torch.arange(n_mentions, n_mentions+len(spans['span_doc_id']), device=device),\n",
    "#                     \"begin\": spans[\"span_begin\"],\n",
    "#                     \"end\": spans[\"span_end\"],\n",
    "#                     \"ner_label\": spans[\"span_label\"],\n",
    "#                     \"@sentence_id\": spans[\"span_doc_id\"],\n",
    "#                 },\n",
    "#                 \"sentence\": dict(batch[\"sentence\", [\"sentence_id\", \"doc_id\"]]),\n",
    "#                 \"doc\": dict(batch[\"doc\"])}, \n",
    "#                 check=False)\n",
    "#             pred_batches.append(pred_batch)\n",
    "#             n_mentions += len(spans['span_doc_id'])\n",
    "            \n",
    "#             sampled_f1 = torch_f1(target_tags[0], torch.stack(sampled_tags).squeeze().permute(1,2,0))\n",
    "#             baseline_f1 = torch_f1(target_tags[0], torch.stack(baseline_tags).squeeze().permute(1,2,0))\n",
    "            \n",
    "#             total_train_ner_loss += float(ner_loss) * len(batch[\"sentence\"])\n",
    "#             total_train_ner_size += len(batch[\"sentence\"])\n",
    "            \n",
    "#             loss = ner_loss * ((sampled_f1 - baseline_f1) or 1)\n",
    "            \n",
    "#             # Perform optimization step\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(all_nets.parameters(), max_grad_norm)\n",
    "#             optim.step()\n",
    "            \n",
    "#             bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "#     from nlstruct.scoring import compute_metrics, merge_pred_and_gold\n",
    "    \n",
    "#     train_pred = Batcher.concat(pred_batches)\n",
    "#     val_pred = extract_mentions(val_batcher, all_nets=all_nets)\n",
    "    \n",
    "#     train_metrics = compute_metrics(merge_pred_and_gold(\n",
    "#         pred=pd.DataFrame(dict(train_pred[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), #extract_mentions(val_batcher, all_nets=all_nets)\n",
    "#         gold=pd.DataFrame(dict(level_train_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), \n",
    "#         span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "#         on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"]), prefix='train_')[[\"train_recall\", \"train_precision\", \"train_f1\"]].to_dict()\n",
    "\n",
    "#     val_metrics = compute_metrics(merge_pred_and_gold(\n",
    "#         pred=pd.DataFrame(dict(val_pred[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), #extract_mentions(val_batcher, all_nets=all_nets)\n",
    "#         gold=pd.DataFrame(dict(level_val_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), \n",
    "#         span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "#         on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"]), prefix='val_')[[\"val_recall\", \"val_precision\", \"val_f1\"]].to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \n",
    "(to be fair, avoid executing this part of the notebook to often, or use the training set instead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
