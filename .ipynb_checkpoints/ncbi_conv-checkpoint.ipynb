{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import *\n",
    "from nlstruct.dataloaders.ncbi_disease import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.text import huggingface_tokenize, regex_sentencize, partition_spans, encode_as_tag, split_into_spans, apply_substitutions, apply_deltas\n",
    "from nlstruct.dataloaders import load_from_brat, load_genia_ner\n",
    "from nlstruct.collections import Dataset, Batcher\n",
    "from nlstruct.utils import merge_with_spans, normalize_vocabularies, factorize_rows, df_to_csr, factorize, torch_global as tg\n",
    "# from nlstruct.modules.crf import BIODecoder, BIOULDecoder\n",
    "from nlstruct.environment import root, cached\n",
    "from nlstruct.train import seed_all\n",
    "from itertools import chain, repeat\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from nlstruct.utils import encode_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bit of code for conll to ann\n",
    "\n",
    "# full_text = ' '.join(a[0]['text'])\n",
    "\n",
    "# cpt_car = 0\n",
    "\n",
    "# mention_id = 0\n",
    "# mention_begin = 0\n",
    "# mention_end = 0\n",
    "# mention_label = ''\n",
    "# mention_text = ''\n",
    "\n",
    "# # T1\tsosy 95 107\tvomissements\n",
    "# # DUMP vocs for prediction\n",
    "# ann_str = ''\n",
    "\n",
    "# for t, g in zip(a[0]['text'], a[0]['gold_labels']):\n",
    "    \n",
    "    \n",
    "#     if mention_text != '' and g[0] in ['B', 'O']:\n",
    "#         mention_end = cpt_car - 1\n",
    "\n",
    "#         ann_str += f'T{mention_id}\\t{mention_label} {mention_begin} {mention_end}\\t{mention_text.strip(\" \")}\\n'\n",
    "\n",
    "#         mention_text = ''\n",
    "#         mention_id += 1\n",
    "        \n",
    "#     if g[0] == 'B':\n",
    "#         mention_begin = cpt_car\n",
    "#         mention_text = t + ' '\n",
    "#         mention_label = g[2:]\n",
    "        \n",
    "#     elif g[0] == 'I':\n",
    "#         mention_text += t + ' '\n",
    "            \n",
    "#     cpt_car += len(t) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@cached\n",
    "def preprocess_train(\n",
    "    dataset,\n",
    "    max_sentence_length,\n",
    "    bert_name,\n",
    "    ner_labels=None,\n",
    "    unknown_labels=\"drop\",\n",
    "    vocabularies=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: Dataset\n",
    "        max_sentence_length: int\n",
    "            Max number of \"words\" as defined by the regex in regex_sentencize (so this is not the nb of wordpieces)\n",
    "        bert_name: str\n",
    "            bert path/name\n",
    "        ner_labels: list of str \n",
    "            allowed ner labels (to be dropped or filtered)\n",
    "        unknown_labels: str\n",
    "            \"drop\" or \"raise\"\n",
    "        vocabularies: dict[str; np.ndarray or list]\n",
    "    Returns\n",
    "    -------\n",
    "    (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, dict[str; np.ndarray or list])\n",
    "        docs:      ('split', 'text', 'doc_id')\n",
    "        sentences: ('split', 'doc_id', 'sentence_idx', 'begin', 'end', 'text', 'sentence_id')\n",
    "        mentions:  ('ner_label', 'doc_id', 'sentence_id', 'mention_id', 'depth', 'text', 'mention_idx', 'begin', 'end')\n",
    "        tokens:    ('split', 'token', 'sentence_id', 'token_id', 'token_idx', 'begin', 'end', 'doc_id', 'sentence_idx')\n",
    "        deltas:    ('doc_id', 'begin', 'end', 'delta')\n",
    "        vocs: vocabularies to be reused later for encoding more data or decoding predictions\n",
    "    \"\"\"\n",
    "    print(\"Dataset:\", dataset)\n",
    "    mentions = dataset[\"mentions\"].rename({\"label\": \"ner_label\"}, axis=1)\n",
    "    \n",
    "    if ner_labels is not None:\n",
    "        len_before = len(mentions)\n",
    "        unknown_ner_labels = list(mentions[~mentions[\"ner_label\"].isin(ner_labels)][\"ner_label\"].drop_duplicates())\n",
    "        \n",
    "        mentions = mentions[mentions[\"ner_label\"].isin(ner_labels)]\n",
    "        if len(unknown_ner_labels) and unknown_labels == \"raise\":\n",
    "            raise Exception(f\"Unkown labels in {len_before-len(mentions)} mentions: \", unknown_ner_labels)\n",
    "\n",
    "    # Check that there is no mention overlap\n",
    "#     mentions = mentions.merge(dataset[\"fragments\"].groupby([\"doc_id\", \"mention_id\"], as_index=False, observed=True).agg({\"begin\": \"min\", \"end\": \"max\"}))\n",
    "    \n",
    "    print(\"Transform texts...\", end=\" \")\n",
    "    docs, deltas = apply_substitutions(\n",
    "        dataset[\"docs\"], *zip(\n",
    "            #(r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "            #(r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "            (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "            (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "        ), apply_unidecode=True)\n",
    "    docs = docs.astype({\"text\": str})\n",
    "    transformed_mentions = apply_deltas(mentions, deltas, on=['doc_id'])\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Splitting into sentences...\", end=\" \")\n",
    "    sentences = regex_sentencize(\n",
    "        docs, \n",
    "        reg_split=r\"(?<=[.])(\\s*\\n+)|(?=, [0-9]\\))\",\n",
    "        min_sentence_length=None, max_sentence_length=max_sentence_length,\n",
    "        balance_parentheses=False,\n",
    "        # balance_parentheses=True, # default is True\n",
    "    )\n",
    "          \n",
    "    [mentions], sentences, sentence_to_docs = partition_spans([transformed_mentions], sentences, new_id_name=\"sentence_id\", overlap_policy=False)\n",
    "    \n",
    "\n",
    "    mentions = mentions[(mentions['begin']>0) & (mentions['end']>0)]\n",
    "    \n",
    "    n_sentences_per_mention = mentions.assign(count=1).groupby([\"doc_id\", \"mention_id\"], as_index=False).agg({\"count\": \"sum\", \"text\": tuple, \"sentence_id\": tuple})\n",
    "    if n_sentences_per_mention[\"count\"].max() > 1:\n",
    "        display(n_sentences_per_mention.query(\"count > 1\"))\n",
    "        display(sentences[sentences[\"sentence_id\"].isin(n_sentences_per_mention.query(\"count > 1\")[\"sentence_id\"].explode())][\"text\"].tolist())\n",
    "        raise Exception(\"Some mentions could be mapped to more than 1 sentences ({})\".format(n_sentences_per_mention[\"count\"].max()))\n",
    "    if sentence_to_docs is not None:\n",
    "        mentions = mentions.merge(sentence_to_docs)\n",
    "        \n",
    "    mentions = mentions.assign(mention_idx=0).nlstruct.groupby_assign([\"doc_id\", \"sentence_id\"], {\"mention_idx\": lambda x: tuple(range(len(x)))})\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Tokenizing...\", end=\" \")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    sentences[\"text\"] = sentences[\"text\"].str.lower()\n",
    "    tokens = huggingface_tokenize(sentences, tokenizer, doc_id_col=\"sentence_id\")\n",
    "\n",
    "    prep = Dataset(docs=docs, sentences=sentences, mentions=mentions, tokens=tokens).copy()    \n",
    "    \n",
    "    mentions = split_into_spans(mentions, tokens, pos_col=\"token_idx\", overlap_policy=False)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Processing nestings (overlapping areas)...\", end=\" \")\n",
    "    # Extract overlapping spans\n",
    "    conflicts = (\n",
    "        merge_with_spans(mentions, mentions, on=[\"doc_id\", \"sentence_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "    )\n",
    "    # ids1, and ids2 make the edges of the overlapping mentions of the same type (see the \"ner_label\")\n",
    "    [ids1, ids2], unique_ids = factorize_rows(\n",
    "        [conflicts[[\"doc_id\", \"sentence_id\", \"mention_id\"]], \n",
    "         conflicts[[\"doc_id\", \"sentence_id\", \"mention_id_other\"]]],\n",
    "        mentions.eval(\"size=(end-begin)\").sort_values(\"size\")[[\"doc_id\", \"sentence_id\", \"mention_id\"]]\n",
    "    )\n",
    "    g = nx.from_scipy_sparse_matrix(df_to_csr(ids1, ids2, n_rows=len(unique_ids), n_cols=len(unique_ids)))\n",
    "    colored_nodes = np.asarray(list(nx.coloring.greedy_color(g, strategy=keep_order).items()))\n",
    "    unique_ids['depth'] = colored_nodes[:, 1][colored_nodes[:, 0].argsort()]\n",
    "    mentions = mentions.merge(unique_ids)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Computing vocabularies...\")\n",
    "    [docs, sentences, mentions, tokens], vocs = normalize_vocabularies(\n",
    "        [docs, sentences, mentions, tokens], \n",
    "        vocabularies={\"split\": [\"train\", \"val\", \"test\"]} if vocabularies is None else vocabularies,\n",
    "        train_vocabularies={\"source\": False, \"text\": False} if vocabularies is None else False,\n",
    "        verbose=True)\n",
    "    print(\"done\")\n",
    "    \n",
    "    unique_mention_ids = encode_ids([mentions], (\"doc_id\", \"sentence_id\", \"mention_id\"))\n",
    "    unique_sentence_ids = encode_ids([sentences, mentions, tokens], (\"doc_id\", \"sentence_id\"))\n",
    "    unique_doc_ids = encode_ids([docs, sentences, mentions, tokens], (\"doc_id\",))\n",
    "    \n",
    "    batcher = Batcher({\n",
    "        \"mention\": {\n",
    "            \"mention_id\": mentions[\"mention_id\"],\n",
    "            \"sentence_id\": mentions[\"sentence_id\"],\n",
    "            \"doc_id\": mentions[\"doc_id\"],\n",
    "            \"begin\": mentions[\"begin\"],\n",
    "            \"end\": mentions[\"end\"],\n",
    "            \"depth\": mentions[\"depth\"],\n",
    "            \"ner_label\": mentions[\"ner_label\"].cat.codes,\n",
    "        },\n",
    "        \"sentence\": {\n",
    "            \"sentence_id\": sentences[\"sentence_id\"],\n",
    "            \"doc_id\": sentences[\"doc_id\"],\n",
    "            \"mention_id\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], mentions[\"mention_id\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"mention_mask\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"token\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], tokens[\"token\"].cat.codes, n_rows=len(unique_sentence_ids)),\n",
    "            \"token_mask\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "        },\n",
    "        \"doc\": {\n",
    "            \"doc_id\": np.arange(len(unique_doc_ids)),\n",
    "            \"sentence_id\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], sentences[\"sentence_id\"], n_rows=len(unique_doc_ids)),\n",
    "            \"sentence_mask\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], n_rows=len(unique_doc_ids)),\n",
    "            \"split\": docs[\"split\"].cat.codes,\n",
    "        }},\n",
    "        masks={\"sentence\": {\"token\": \"token_mask\", \"mention_id\": \"mention_mask\"}, \n",
    "               \"doc\": {\"sentence_id\": \"sentence_mask\"}}\n",
    "    )\n",
    "    \n",
    "    return batcher, prep, deltas, vocs\n",
    "\n",
    "def keep_order(G, colors):\n",
    "    \"\"\"Returns a list of the nodes of ``G`` in ordered identically to their id in the graph\n",
    "    ``G`` is a NetworkX graph. ``colors`` is ignored.\n",
    "    This is to assign a depth using the nx.coloring.greedy_color function\n",
    "    \"\"\"\n",
    "    return sorted(list(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        doc_id  mention_id         category  \\\n",
      "0     10192393  10192393-0     DiseaseClass   \n",
      "1     10192393  10192393-1     DiseaseClass   \n",
      "2     10192393  10192393-2     DiseaseClass   \n",
      "3     10192393  10192393-3  SpecificDisease   \n",
      "4     10192393  10192393-4  SpecificDisease   \n",
      "...        ...         ...              ...   \n",
      "6876   8696339   8696339-0  SpecificDisease   \n",
      "6877   8696339   8696339-1  SpecificDisease   \n",
      "6878   8696339   8696339-2         Modifier   \n",
      "6879   8696339   8696339-3  SpecificDisease   \n",
      "6880   8696339   8696339-4     DiseaseClass   \n",
      "\n",
      "                                     text  \n",
      "0                             skin tumour  \n",
      "1                                  cancer  \n",
      "2                           colon cancers  \n",
      "3              adenomatous polyposis coli  \n",
      "4                                     APC  \n",
      "...                                   ...  \n",
      "6876                   Huntington disease  \n",
      "6877                                   HD  \n",
      "6878                                   HD  \n",
      "6879                                   HD  \n",
      "6880  disorder of inappropriate apoptosis  \n",
      "\n",
      "[6881 rows x 4 columns]\n",
      "Dataset: Dataset(\n",
      "  (docs):       792 * ('doc_id', 'text', 'split')\n",
      "  (mentions):  7059 * ('doc_id', 'mention_id', 'category', 'text', 'begin', 'end', 'label_id', 'label', 'norm_label')\n",
      "  (labels):    7059 * ('label_id', 'doc_id', 'mention_id', 'label')\n",
      "  (fragments): 6881 * ('doc_id', 'mention_id', 'begin', 'end', 'fragment_id')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... done\n",
      "Tokenizing... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/nlstruct/text/chunking/huggingface.py:11: FutureWarning: doc_id_col is not used anymore in the huggingface_tokenize function\n",
      "  warnings.warn(\"doc_id_col is not used anymore in the huggingface_tokenize function\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Processing nestings (overlapping areas)... done\n",
      "Computing vocabularies...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-base-cased\"\n",
    "dataset = load_ncbi_disease()\n",
    "\n",
    "print(dataset['mentions'])\n",
    "\n",
    "docs = dataset['docs']\n",
    "\n",
    "keep_n_first = None\n",
    "\n",
    "if keep_n_first:\n",
    "    docs = docs[:keep_n_first]\n",
    "    first_ids = docs['doc_id']\n",
    "    \n",
    "    first_mentions = dataset[\"mentions\"].loc[dataset[\"mentions\"]['doc_id'].isin(first_ids)]\n",
    "    first_fragments = dataset[\"fragments\"].loc[dataset[\"fragments\"]['doc_id'].isin(first_ids)]\n",
    "    first_attributes = dataset[\"attributes\"].loc[dataset[\"attributes\"]['doc_id'].isin(first_ids)]\n",
    "    \n",
    "    dataset[\"mentions\"] = first_mentions\n",
    "    dataset[\"fragments\"] = first_fragments\n",
    "    dataset[\"attributes\"] = first_attributes\n",
    "\n",
    "dataset['mentions'] = dataset['mentions'].merge(dataset[\"fragments\"].groupby([\"doc_id\", \"mention_id\"], as_index=False, observed=True).agg({\"begin\": \"min\", \"end\": \"max\"}))\n",
    "\n",
    "merged = dataset['mentions'].merge(dataset['labels'], on=['doc_id', 'mention_id'])\n",
    "merged['mention_id'] = merged['label_id']\n",
    "merged['norm_label'] = merged['label'] # rename norm labels\n",
    "merged['label'] = merged['category']\n",
    "    \n",
    "dataset['docs'] = docs\n",
    "\n",
    "dataset['mentions'] = merged\n",
    "\n",
    "ner_labels = list(dataset['mentions']['label'].unique())\n",
    "\n",
    "batcher, prep, deltas, vocs = preprocess_train(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=140,\n",
    "    bert_name=bert_name,\n",
    "    ner_labels= ner_labels,\n",
    "    unknown_labels=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from pathlib import Path \n",
    "\n",
    "split = 'all'\n",
    "\n",
    "all_sentences = prep['sentences']#[prep['sentences']['split']==split]\n",
    "all_mentions = prep['mentions']\n",
    "\n",
    "def dataset_to_ann(\n",
    "    sentences, \n",
    "    mentions,\n",
    "    ann_path,\n",
    "):\n",
    "    \n",
    "    Path(ann_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for doc_id in all_sentences['doc_id'].unique():\n",
    "        \n",
    "        doc_char_count = 0\n",
    "        \n",
    "        doc_sentences = all_sentences[all_sentences.doc_id==doc_id]\n",
    "        \n",
    "        doc_str = ' '.join([d['text'] for _,d in doc_sentences.iterrows()])\n",
    "        ann_str = ''\n",
    "        mention_id = 0\n",
    "        \n",
    "        for _, sentence in doc_sentences.iterrows():\n",
    "            sentence_id = sentence['sentence_id']\n",
    "            first_mentions = all_mentions[all_mentions['sentence_id']==sentence_id]\n",
    "            first_mentions['begin'] += doc_char_count \n",
    "            first_mentions['end'] += doc_char_count \n",
    "            \n",
    "            for _, fm in first_mentions.iterrows():\n",
    "                ann_str += f\"T{mention_id}\\t{fm['norm_label']} {fm['begin']} {fm['end']}\\t{fm['text']}\\n\"\n",
    "                mention_id += 1\n",
    "                \n",
    "            doc_char_count += len(sentence['text']) + 1\n",
    "\n",
    "        with open(path.join(ann_path, f'{doc_id}.ann'), 'w') as f:\n",
    "            f.write(ann_str)\n",
    "                \n",
    "        with open(path.join(ann_path, f'{doc_id}.txt'), 'w') as f:\n",
    "            f.write(doc_str)\n",
    "        \n",
    "dataset_to_ann(all_sentences, all_mentions,\n",
    "    f\"/home/ytaille/data/resources/medic/standoff_cui/{split}/\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_conll():\n",
    "\n",
    "    split = 'dev'\n",
    "\n",
    "    text_file = f'ncbi_conll_ner_{split}.conll'\n",
    "    all_sentences = prep['sentences'][prep['sentences']['split']==split]\n",
    "    all_mentions = prep['mentions']#[prep['mentions']['split']==split]\n",
    "\n",
    "    all_text = []\n",
    "\n",
    "    for _, sentence in all_sentences.iterrows():\n",
    "        sentence_id = sentence['sentence_id']\n",
    "        first_mentions = all_mentions[all_mentions['sentence_id']==sentence_id]\n",
    "\n",
    "        if sentence['text'][-2::] != ' .':\n",
    "            sentence['text'] = sentence['text'][:-1] + ' .'\n",
    "\n",
    "        sentence['text'] = sentence['text'].replace('\\n', ' ')\n",
    "\n",
    "        i = 0\n",
    "        text = []\n",
    "        mentions = []\n",
    "        norm_mentions = []\n",
    "\n",
    "        already_seen = []\n",
    "\n",
    "        if len(first_mentions) == 0:\n",
    "            bef_text = sentence['text'].split(' ')\n",
    "            text.extend(bef_text)\n",
    "            mentions.extend(['O'] * len(bef_text))\n",
    "            norm_mentions.extend(['O'] * len(bef_text))\n",
    "        else:\n",
    "            for _, fm in first_mentions.iterrows():\n",
    "                begin, end, mention_text, label, norm_label = fm[['begin', 'end', 'text', 'ner_label', 'norm_label']]\n",
    "\n",
    "                if any([(begin in a) or (end in a) for a in already_seen]):\n",
    "                    continue\n",
    "\n",
    "                already_seen.append(range(begin, end))\n",
    "\n",
    "                bef_text = sentence['text'][i:begin].split(' ')\n",
    "                bef_mentions = ['O'] * len(bef_text)\n",
    "\n",
    "                text.extend(bef_text)\n",
    "                mentions.extend(bef_mentions)\n",
    "                norm_mentions.extend(bef_mentions)\n",
    "\n",
    "                mention_text = mention_text.split(' ')\n",
    "                text.extend(mention_text)\n",
    "                mentions.extend(['B-' + label] + ['I-' + label] * (len(mention_text)-1))\n",
    "                norm_mentions.extend(['B-' + norm_label] + ['I-' + norm_label] * (len(mention_text)-1))\n",
    "                i = end\n",
    "\n",
    "        after_text = sentence['text'][end:].split(' ')\n",
    "        text.extend(after_text)\n",
    "        mentions.extend(['O'] * len(after_text))\n",
    "        norm_mentions.extend(['O'] * len(after_text))\n",
    "\n",
    "        all_text.extend([f'{t} NN {n} {m}\\n' for t,m,n in zip(text, mentions, norm_mentions) if (t!='' and t!='\\n')] + ['\\n'])\n",
    "\n",
    "    with open(text_file, 'w') as f:\n",
    "        f.write('-DOCSTART- -X- -X- O\\n\\n')\n",
    "        f.write(''.join(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
